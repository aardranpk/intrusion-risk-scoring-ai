# ğŸ” Explainable Intrusion Risk Scoring System (CICIDS)

![Python](https://img.shields.io/badge/Python-3.x-blue?logo=python&logoColor=white)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-ML-orange)
![XGBoost](https://img.shields.io/badge/XGBoost-GradientBoosting-red)
![SHAP](https://img.shields.io/badge/Explainable%20AI-SHAP-purple)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B?logo=streamlit&logoColor=white)
![Security](https://img.shields.io/badge/Cybersecurity-Intrusion%20Detection-darkgreen)
[![Author](https://img.shields.io/badge/Author-Aardran%20Premakumar-blueviolet)](https://github.com/<your-username>)


An **end-to-end, explainable AI system** for network intrusion detection that goes beyond binary alerts by producing **risk scores, severity levels, and human-interpretable explanations** for security analysts.

This project demonstrates how machine learning can support **real-world SOC (Security Operations Center) decision-making**, not just model accuracy.

---

## ğŸš¨ Problem Statement

Traditional intrusion detection systems often generate **binary alerts** (attack / no attack), which leads to:

- Alert fatigue for security analysts  
- Poor prioritization of threats  
- Lack of transparency in ML-driven decisions  

In real security operations, analysts need answers to:
- *How risky is this event?*
- *Why was it flagged?*
- *What should I investigate first?*

---

## ğŸ¯ Solution Overview

This project implements an **Explainable Intrusion Risk Scoring System** that:

- Detects malicious network traffic using **XGBoost**
- Outputs a **risk score (0â€“100)** instead of a binary label
- Assigns **severity levels** (Low / Medium / High / Critical)
- Explains predictions using **SHAP feature attributions**
- Provides an **interactive Streamlit dashboard** for analysts

---

## ğŸ§  Key Features

- Risk-based scoring rather than yes/no predictions  
- Explainable AI (XAI) using SHAP for transparency  
- Imbalance-aware modeling for realistic attack detection  
- Analyst-friendly dashboard for exploration and triage  
- Production-style project structure (no notebooks required)

---

## ğŸ—ï¸ Architecture

The system follows a production-style ML pipeline designed for SOC workflows:

Network Flow Data (CICIDS)
<br>â”‚
<br>â–¼
<br>Data Loader & Preprocessing
<br>â”‚
<br>â–¼
<br>XGBoost Classifier
<br>â”‚
<br>â”œâ”€â”€ Probability (0â€“1)
<br>â”‚
<br>â”œâ”€â”€ Risk Score (0â€“100)
<br>â”‚ â””â”€â”€ Severity Mapping
<br>â”‚
<br>â””â”€â”€ SHAP Explainer
<br>â”‚
<br>â–¼
<br>Analyst Dashboard (Streamlit)


---

## ğŸ“Š Dataset

- **CICIDS 2017** (Canadian Institute for Cybersecurity)
- Network flow-level features
- Includes benign traffic and multiple attack scenarios  
- Dataset files are **not committed** (handled via `.gitignore`)

---

## ğŸ› ï¸ Tech Stack

- Python 3
- Pandas, NumPy
- Scikit-learn
- XGBoost
- SHAP
- Streamlit
- Joblib

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/aardranpk/intrusion-risk-scoring-ai.git
cd intrusion-risk-scoring-ai

```
### 2ï¸âƒ£Create & activate virtual environment
```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1

```
### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt

```
## ğŸ“ Data Setup

Create a data/ folder in the project root and place one CICIDS parquet file inside:

data/
 â””â”€â”€ Portscan-Friday-no-metadata.parquet
Dataset files are excluded from version control.

## ğŸš€ Running the Project

#### Train baseline model
```bash
python -m src.models.train_baseline

```

#### Train XGBoost risk scoring model
```bash
python -m src.models.train_xgb

```

#### Run risk scoring demo
```bash
python -m src.models.demo_risk_score

```

#### Run SHAP explanation demo
```bash
python -m src.explain.demo_explain


```

#### Launch dashboard
```bash
python -m streamlit run dashboard/app.py

```

## ğŸ“ˆ Example Results

- ROC-AUC â‰ˆ **0.999**
- High recall for attack traffic
- Risk score + severity per network flow
- Top contributing features for every prediction

---

## ğŸ” Explainability Example (SHAP)

Fwd Packet Length Mean â†’ â†“ risk
<br>Init Bwd Win Bytes â†’ â†‘ risk
<br>Total Backward Packets â†’ â†“ risk


This allows analysts to **understand and trust** model decisions.

---

## âš ï¸ Limitations

- Single dataset source (CICIDS)
- No real-time streaming ingestion
- No analyst feedback loop

---

## ğŸ”® Future Improvements

- Multi-class attack classification
- Real-time log ingestion (streaming)
- Analyst feedback loop for retraining
- Model drift detection
- Authentication & RBAC for dashboard

---

## ğŸ’¼ Resume Highlight

> Built an explainable AI-based intrusion risk scoring system using XGBoost and SHAP, enabling risk-based prioritization of network traffic with transparent, analyst-facing explanations.

---

## ğŸ“Œ Why This Project Matters

This project focuses on **how AI is used in security operations**, not just model accuracy.  
It demonstrates:

- Systems thinking
- Explainable ML
- Practical security trade-offs
- Production-ready engineering practices

---

## ğŸ‘¤ Author

**Aardran Premakumar**  
AI / ML | Security Analytics
